Pipeline Log Initiated

[2025-11-01 17:42:47] ======================================================================
[2025-11-01 17:42:47] üöÄ Starting Full Data Pipeline
[2025-11-01 17:42:47] ======================================================================
[2025-11-01 17:42:47] 
üîπ Running Step: Data Collection
[2025-11-01 17:42:47] Starting data_collection.py (Attempt 1)...
Fetching data from r/legaltech (Category: Law)...
Fetching data from r/LawFirm (Category: Law)...
[2025-11-01 17:52:05] ======================================================================
[2025-11-01 17:52:05] üöÄ Starting Full Data Pipeline
[2025-11-01 17:52:05] ======================================================================
[2025-11-01 17:52:05] 
üîπ Running Step: Data Collection
[2025-11-01 17:52:05] Starting data_collection.py (Attempt 1)...
Traceback (most recent call last):
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_collection.py", line 163, in <module>
    main()
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_collection.py", line 133, in main
    log("\U0001f680 Starting Reddit data collection...")
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_collection.py", line 38, in log
    print(msg, flush=True)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f680' in position 22: character maps to <undefined>
[2025-11-01 17:52:07] ‚ö†Ô∏è data_collection.py failed (Attempt 1/2). Retrying...
[2025-11-01 17:52:12] Starting data_collection.py (Attempt 2)...
Traceback (most recent call last):
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_collection.py", line 163, in <module>
    main()
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_collection.py", line 133, in main
    log("\U0001f680 Starting Reddit data collection...")
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_collection.py", line 38, in log
    print(msg, flush=True)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f680' in position 22: character maps to <undefined>
[2025-11-01 17:52:14] ‚ö†Ô∏è data_collection.py failed (Attempt 2/2). Retrying...
[2025-11-01 17:52:19] ‚ùå data_collection.py failed after 2 attempts. Moving to next step.
[2025-11-01 17:52:19] ‚ö†Ô∏è Step 'Data Collection' encountered an error. Continuing to next...

[2025-11-01 17:52:19] 
üîπ Running Step: Data Cleaning
[2025-11-01 17:52:19] Starting data_clean.py (Attempt 1)...
Traceback (most recent call last):
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_clean.py", line 11, in <module>
    df = pd.read_csv("reddit_data.csv")
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'reddit_data.csv'
[2025-11-01 17:52:22] ‚ö†Ô∏è data_clean.py failed (Attempt 1/2). Retrying...
[2025-11-01 17:52:27] Starting data_clean.py (Attempt 2)...
Traceback (most recent call last):
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_clean.py", line 11, in <module>
    df = pd.read_csv("reddit_data.csv")
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'reddit_data.csv'
[2025-11-01 17:52:30] ‚ö†Ô∏è data_clean.py failed (Attempt 2/2). Retrying...
[2025-11-01 17:52:35] ‚ùå data_clean.py failed after 2 attempts. Moving to next step.
[2025-11-01 17:52:35] ‚ö†Ô∏è Step 'Data Cleaning' encountered an error. Continuing to next...

[2025-11-01 17:52:35] 
üîπ Running Step: Sentiment Analysis
[2025-11-01 17:52:35] Starting data_sentiment.py (Attempt 1)...
[nltk_data] Downloading package vader_lexicon to
[nltk_data]     C:\Users\jonis/nltk_data...
[nltk_data]   Package vader_lexicon is already up-to-date!
[nltk_data] Downloading package punkt to C:\Users\jonis/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\jonis/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Traceback (most recent call last):
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_sentiment.py", line 25, in <module>
    df_clean = pd.read_csv("reddit_data_clean.csv")
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'reddit_data_clean.csv'
[2025-11-01 17:52:39] ‚ö†Ô∏è data_sentiment.py failed (Attempt 1/2). Retrying...
[2025-11-01 17:52:44] Starting data_sentiment.py (Attempt 2)...
[nltk_data] Downloading package vader_lexicon to
[nltk_data]     C:\Users\jonis/nltk_data...
[nltk_data]   Package vader_lexicon is already up-to-date!
[nltk_data] Downloading package punkt to C:\Users\jonis/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\jonis/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Traceback (most recent call last):
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_sentiment.py", line 25, in <module>
    df_clean = pd.read_csv("reddit_data_clean.csv")
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'reddit_data_clean.csv'
[2025-11-01 17:52:48] ‚ö†Ô∏è data_sentiment.py failed (Attempt 2/2). Retrying...
[2025-11-01 17:52:53] ‚ùå data_sentiment.py failed after 2 attempts. Moving to next step.
[2025-11-01 17:52:53] ‚ö†Ô∏è Step 'Sentiment Analysis' encountered an error. Continuing to next...

[2025-11-01 17:52:53] 
üîπ Running Step: Indexing to Pinecone
[2025-11-01 17:52:53] Starting store_index.py (Attempt 1)...
C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\langchain_pinecone\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.

For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`
with: `from pydantic import BaseModel`
or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. 	from pydantic.v1 import BaseModel

  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\store_index.py:8: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:

>> from langchain.embeddings import HuggingFaceEmbeddings

with new imports of:

>> from langchain_community.embeddings import HuggingFaceEmbeddings
You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>
  from langchain.embeddings import HuggingFaceEmbeddings
 Loading environment variables...
 Connecting to Pinecone...
 File not found: reddit_data_sentiment.csv
[2025-11-01 17:53:05] ‚ö†Ô∏è store_index.py failed (Attempt 1/2). Retrying...
[2025-11-01 17:53:10] Starting store_index.py (Attempt 2)...
C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\langchain_pinecone\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.

For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`
with: `from pydantic import BaseModel`
or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. 	from pydantic.v1 import BaseModel

  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\store_index.py:8: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:

>> from langchain.embeddings import HuggingFaceEmbeddings

with new imports of:

>> from langchain_community.embeddings import HuggingFaceEmbeddings
You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>
  from langchain.embeddings import HuggingFaceEmbeddings
 Loading environment variables...
 Connecting to Pinecone...
 File not found: reddit_data_sentiment.csv
[2025-11-01 17:53:19] ‚ö†Ô∏è store_index.py failed (Attempt 2/2). Retrying...
[2025-11-01 17:53:24] ‚ùå store_index.py failed after 2 attempts. Moving to next step.
[2025-11-01 17:53:24] ‚ö†Ô∏è Step 'Indexing to Pinecone' encountered an error. Continuing to next...

[2025-11-01 17:53:24] 
‚úÖ Pipeline finished in 1.32 minutes.
[2025-11-01 17:53:24] ======================================================================
[2025-11-01 17:53:38] ======================================================================
[2025-11-01 17:53:38]  Starting Full Data Pipeline
[2025-11-01 17:53:38] ======================================================================
[2025-11-01 17:53:38] 
üîπ Running Step: Data Collection
[2025-11-01 17:53:38] Starting data_collection.py (Attempt 1)...
[2025-11-01 17:53:40]  Starting Reddit data collection...
[2025-11-01 17:53:40]  Fetching from r/legaltech (Category: Law)...
[2025-11-01 17:54:04]  Fetched 1059 new records from r/legaltech
[2025-11-01 17:54:12]  Fetching from r/LawFirm (Category: Law)...
[2025-11-01 17:54:39]  Fetched 1039 new records from r/LawFirm
[2025-11-01 17:54:47]  Fetching from r/Law (Category: Law)...
[2025-11-01 17:55:34]  Fetched 975 new records from r/Law
[2025-11-01 17:55:42]  Fetching from r/LegalAdvice (Category: Law)...
[2025-11-01 17:56:02]  Fetched 473 new records from r/LegalAdvice
[2025-11-01 17:56:10]  Fetching from r/LegalAdviceUK (Category: Law)...
[2025-11-01 17:56:36]  Fetched 830 new records from r/LegalAdviceUK
[2025-11-01 17:56:44]  Fetching from r/LegalAdviceCanada (Category: Law)...
[2025-11-01 17:57:09]  Fetched 942 new records from r/LegalAdviceCanada
[2025-11-01 17:57:17]  Fetching from r/Paralegal (Category: Law)...
[2025-11-01 17:57:41]  Fetched 857 new records from r/Paralegal
[2025-11-01 17:57:49]  Fetching from r/LawSchool (Category: Law)...
[2025-11-01 17:58:12]  Fetched 835 new records from r/LawSchool
[2025-11-01 17:58:20]  Fetching from r/LegalNews (Category: Law)...
[2025-11-01 17:59:08]  Fetched 772 new records from r/LegalNews
[2025-11-01 17:59:16]  Fetching from r/Construction (Category: Construction)...
[2025-11-01 18:00:08]  Fetched 856 new records from r/Construction
[2025-11-01 18:00:16]  Fetching from r/Contractors (Category: Construction)...
[2025-11-01 18:00:17]  Fetched 0 new records from r/Contractors
[2025-11-01 18:00:25]  Fetching from r/HomeImprovement (Category: Construction)...
[2025-11-01 18:01:10]  Fetched 455 new records from r/HomeImprovement
[2025-11-01 18:01:18]  Fetching from r/DIY (Category: Construction)...
[2025-11-01 18:02:10]  Fetched 910 new records from r/DIY
[2025-11-01 18:02:18]  Fetching from r/DIYChatRoom (Category: Construction)...
[2025-11-01 18:02:18]  Skipping r/DIYChatRoom due to error: Redirect to /subreddits/search
[2025-11-01 18:02:26]  Fetching from r/Electrical (Category: Construction)...
[2025-11-01 18:03:11]  Fetched 675 new records from r/Electrical
[2025-11-01 18:03:19]  Fetching from r/Plumbing (Category: Construction)...
[2025-11-01 18:04:11]  Fetched 467 new records from r/Plumbing
[2025-11-01 18:04:19]  Fetching from r/HVAC (Category: Construction)...
[2025-11-01 18:05:12]  Fetched 957 new records from r/HVAC
[2025-11-01 18:05:20]  Fetching from r/AskEngineers (Category: Construction)...
[2025-11-01 18:06:13]  Fetched 1052 new records from r/AskEngineers
[2025-11-01 18:06:21]  Fetching from r/sysadmin (Category: Tech)...
[2025-11-01 18:07:13]  Fetched 940 new records from r/sysadmin
[2025-11-01 18:07:21]  Fetching from r/msp (Category: Tech)...
[2025-11-01 18:08:15]  Fetched 1160 new records from r/msp
[2025-11-01 18:08:23]  Fetching from r/talesfromtechsupport (Category: Tech)...
[2025-11-01 18:08:37]  Fetched 439 new records from r/talesfromtechsupport
[2025-11-01 18:08:45]  Fetching from r/iiiiiiitttttttttttt (Category: Tech)...
[2025-11-01 18:09:21]  Fetched 1127 new records from r/iiiiiiitttttttttttt
[2025-11-01 18:09:29]  Fetching from r/techsupportgore (Category: Tech)...
[2025-11-01 18:09:43]  Fetched 469 new records from r/techsupportgore
[2025-11-01 18:09:51]  Fetching from r/ITCareerQuestions (Category: Tech)...
[2025-11-01 18:10:42]  Fetched 1040 new records from r/ITCareerQuestions
[2025-11-01 18:10:50]  Fetching from r/netsec (Category: Tech)...
[2025-11-01 18:11:26]  Fetched 277 new records from r/netsec
[2025-11-01 18:11:34]  Fetching from r/cybersecurity (Category: Tech)...
[2025-11-01 18:12:27]  Fetched 591 new records from r/cybersecurity
[2025-11-01 18:12:35]  Fetching from r/technology (Category: Tech)...
[2025-11-01 18:13:31]  Fetched 1029 new records from r/technology
[2025-11-01 18:13:39]  Fetching from r/tech (Category: Tech)...
[2025-11-01 18:13:58]  Fetched 547 new records from r/tech
[2025-11-01 18:14:06]  Fetching from r/gadgets (Category: Tech)...
[2025-11-01 18:15:18]  Fetched 1426 new records from r/gadgets
[2025-11-01 18:15:26]  Fetching from r/apple (Category: Tech)...
[2025-11-01 18:16:38]  Fetched 1432 new records from r/apple
[2025-11-01 18:16:46]  Fetching from r/linux (Category: Tech)...
[2025-11-01 18:17:23]  Fetched 1038 new records from r/linux
[2025-11-01 18:17:32]  Saved updated dataset to data\reddit_data.csv ‚Äî total 82165 records.
[2025-11-01 18:17:32]  New rows added: 24669
[2025-11-01 18:17:32]  Data collection completed successfully!
[2025-11-01 18:17:33]  data_collection.py completed successfully.
[2025-11-01 18:17:33] 
üîπ Running Step: Data Cleaning
[2025-11-01 18:17:33] Starting data_clean.py (Attempt 1)...
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_clean.py:92: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.countplot(data=df_clean, x="category", order=df_clean["category"].value_counts().index, palette="Set2")
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_clean.py:101: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=top_keywords["count"], y=top_keywords["keyword"], palette="viridis")
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_clean.py:110: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x=top_subs.values, y=top_subs.index, palette="mako")
[2025-11-01 18:23:54]  data_clean.py completed successfully.
[2025-11-01 18:23:54] 
üîπ Running Step: Sentiment Analysis
[2025-11-01 18:23:54] Starting data_sentiment.py (Attempt 1)...
[nltk_data] Downloading package vader_lexicon to
[nltk_data]     C:\Users\jonis/nltk_data...
[nltk_data]   Package vader_lexicon is already up-to-date!
[nltk_data] Downloading package punkt to C:\Users\jonis/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\jonis/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Saved cleaned + sentiment dataset with 1377 rows to reddit_data_sentiment.csv

Sentiment distribution:
sentiment
positive    930
negative    268
neutral     179
Name: count, dtype: int64

Top words in negative mentions (after stopword removal):
        word  count
0      linux    232
1    windows    145
2        use    107
3       dont     94
4         im     81
5       time     79
6       like     78
7        one     77
8       work     72
9      would     71
10       get     64
11       new     64
12      need     62
13      even     62
14     using     60
15    system     57
16    people     56
17  firewall     54
18      know     52
19       ive     49

Top bigrams in negative mentions:
                  bigram  count
0        (invalid, user)     18
1       (upload, speeds)     17
2   (connection, closed)     11
3      (closed, invalid)     11
4        (port, preauth)     11
5        (linux, distro)      9
6    (operating, system)      9
7          (dont, think)      9
8        (linux, kernel)      7
9       (install, linux)      7
10        (using, linux)      7
11     (developer, mode)      7
12     (windows, server)      7
13        (around, mbps)      7
14     (something, like)      6
15        (even, though)      6
16         (feels, like)      6
17       (refresh, code)      6
18          (dont, want)      6
19          (feel, like)      6
[2025-11-01 18:24:06]  data_sentiment.py completed successfully.
[2025-11-01 18:24:06] 
üîπ Running Step: Indexing to Pinecone
[2025-11-01 18:24:06] Starting store_index.py (Attempt 1)...
C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\langchain_pinecone\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.

For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`
with: `from pydantic import BaseModel`
or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. 	from pydantic.v1 import BaseModel

  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\store_index.py:8: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:

>> from langchain.embeddings import HuggingFaceEmbeddings

with new imports of:

>> from langchain_community.embeddings import HuggingFaceEmbeddings
You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>
  from langchain.embeddings import HuggingFaceEmbeddings
 Loading environment variables...
 Connecting to Pinecone...
 Loaded dataset with 1377 rows.
 Preparing documents...
 Prepared 1375 documents.
 Splitting documents into smaller chunks...
 After splitting: 4611 total chunks ready for embedding.
 Initializing embedding model (MiniLM)...
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\store_index.py:81: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.
  embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
 Index 'reddit-insights' already exists. Using existing index.
 Uploading embeddings to Pinecone...
 Uploaded all embeddings to Pinecone successfully.
 Indexing complete and ready for chatbot use.
[2025-11-01 18:29:00]  store_index.py completed successfully.
[2025-11-01 18:29:00] 
 Pipeline finished in 35.36 minutes.
[2025-11-01 18:29:00] ======================================================================
[2025-11-08 17:04:15] ======================================================================
[2025-11-08 17:04:15] Starting Full Data Pipeline
[2025-11-08 17:04:15] ======================================================================
[2025-11-08 17:04:15] 
üîπ Running Step: Data Collection
[2025-11-08 17:04:15] Starting data_collection.py (Attempt 1)...
[2025-11-08 17:04:19] Starting Reddit data collection with duplicate prevention...
[2025-11-08 17:04:19] Fetching from r/legaltech (Category: Law) [Attempt 1]
[2025-11-08 17:04:27] Retrieved 182 new records from r/legaltech.
[2025-11-08 17:04:35] Fetching from r/LawFirm (Category: Law) [Attempt 1]
[2025-11-08 17:04:47] Retrieved 377 new records from r/LawFirm.
[2025-11-08 17:04:55] Fetching from r/Law (Category: Law) [Attempt 1]
[2025-11-08 17:05:51] Retrieved 1053 new records from r/Law.
[2025-11-08 17:05:59] Fetching from r/LegalAdvice (Category: Law) [Attempt 1]
[2025-11-08 17:06:20] Retrieved 456 new records from r/LegalAdvice.
[2025-11-08 17:06:28] Fetching from r/LegalAdviceUK (Category: Law) [Attempt 1]
[2025-11-08 17:06:55] Retrieved 837 new records from r/LegalAdviceUK.
[2025-11-08 17:07:03] Fetching from r/LegalAdviceCanada (Category: Law) [Attempt 1]
[2025-11-08 17:07:30] Retrieved 890 new records from r/LegalAdviceCanada.
[2025-11-08 17:07:38] Fetching from r/Paralegal (Category: Law) [Attempt 1]
[2025-11-08 17:08:04] Retrieved 782 new records from r/Paralegal.
[2025-11-08 17:08:12] Fetching from r/LawSchool (Category: Law) [Attempt 1]
[2025-11-08 17:08:38] Retrieved 851 new records from r/LawSchool.
[2025-11-08 17:08:46] Fetching from r/LegalNews (Category: Law) [Attempt 1]
[2025-11-08 17:09:23] Retrieved 699 new records from r/LegalNews.
[2025-11-08 17:09:31] Fetching from r/Construction (Category: Construction) [Attempt 1]
[2025-11-08 17:10:07] Retrieved 905 new records from r/Construction.
[2025-11-08 17:10:16] Fetching from r/Contractors (Category: Construction) [Attempt 1]
[2025-11-08 17:10:16]  No new posts found for r/Contractors.
[2025-11-08 17:10:24] Fetching from r/HomeImprovement (Category: Construction) [Attempt 1]
[2025-11-08 17:11:08] Retrieved 476 new records from r/HomeImprovement.
[2025-11-08 17:11:16] Fetching from r/DIY (Category: Construction) [Attempt 1]
[2025-11-08 17:12:08] Retrieved 852 new records from r/DIY.
[2025-11-08 17:12:16] Fetching from r/DIYChatRoom (Category: Construction) [Attempt 1]
[2025-11-08 17:12:16] Attempt 1 failed for r/DIYChatRoom: Redirect to /subreddits/search
[2025-11-08 17:12:26] Fetching from r/DIYChatRoom (Category: Construction) [Attempt 2]
[2025-11-08 17:12:27] Attempt 2 failed for r/DIYChatRoom: Redirect to /subreddits/search
[2025-11-08 17:12:37] Fetching from r/DIYChatRoom (Category: Construction) [Attempt 3]
[2025-11-08 17:12:37] Attempt 3 failed for r/DIYChatRoom: Redirect to /subreddits/search
[2025-11-08 17:12:47] Skipping r/DIYChatRoom after 3 failed attempts.
[2025-11-08 17:12:47] Fetching from r/Electrical (Category: Construction) [Attempt 1]
[2025-11-08 17:13:15] Retrieved 685 new records from r/Electrical.
[2025-11-08 17:13:23] Fetching from r/Plumbing (Category: Construction) [Attempt 1]
[2025-11-08 17:14:11] Retrieved 545 new records from r/Plumbing.
[2025-11-08 17:14:19] Fetching from r/HVAC (Category: Construction) [Attempt 1]
[2025-11-08 17:15:13] Retrieved 1254 new records from r/HVAC.
[2025-11-08 17:15:21] Fetching from r/AskEngineers (Category: Construction) [Attempt 1]
[2025-11-08 17:15:45] Retrieved 450 new records from r/AskEngineers.
[2025-11-08 17:15:53] Fetching from r/sysadmin (Category: Tech) [Attempt 1]
[2025-11-08 17:16:47] Retrieved 961 new records from r/sysadmin.
[2025-11-08 17:16:55] Fetching from r/msp (Category: Tech) [Attempt 1]
[2025-11-08 17:17:47] Retrieved 1044 new records from r/msp.
[2025-11-08 17:17:55] Fetching from r/talesfromtechsupport (Category: Tech) [Attempt 1]
[2025-11-08 17:18:01] Retrieved 112 new records from r/talesfromtechsupport.
[2025-11-08 17:18:09] Fetching from r/iiiiiiitttttttttttt (Category: Tech) [Attempt 1]
[2025-11-08 17:18:22] Retrieved 309 new records from r/iiiiiiitttttttttttt.
[2025-11-08 17:18:30] Fetching from r/techsupportgore (Category: Tech) [Attempt 1]
[2025-11-08 17:18:36] Retrieved 142 new records from r/techsupportgore.
[2025-11-08 17:18:44] Fetching from r/ITCareerQuestions (Category: Tech) [Attempt 1]
[2025-11-08 17:19:21] Retrieved 1013 new records from r/ITCareerQuestions.
[2025-11-08 17:19:29] Fetching from r/netsec (Category: Tech) [Attempt 1]
[2025-11-08 17:19:34] Retrieved 47 new records from r/netsec.
[2025-11-08 17:19:42] Fetching from r/cybersecurity (Category: Tech) [Attempt 1]
[2025-11-08 17:19:59]  Error fetching comments for r/cybersecurity: received 429 HTTP response
[2025-11-08 17:19:59]  Error fetching comments for r/cybersecurity: received 429 HTTP response
[2025-11-08 17:19:59]  Error fetching comments for r/cybersecurity: received 429 HTTP response
[2025-11-08 17:19:59]  Error fetching comments for r/cybersecurity: received 429 HTTP response
[2025-11-08 17:19:59]  Error fetching comments for r/cybersecurity: received 429 HTTP response
[2025-11-08 17:19:59]  Error fetching comments for r/cybersecurity: received 429 HTTP response
[2025-11-08 17:20:24] Retrieved 598 new records from r/cybersecurity.
[2025-11-08 17:20:32] Fetching from r/technology (Category: Tech) [Attempt 1]
[2025-11-08 17:21:41] Retrieved 1118 new records from r/technology.
[2025-11-08 17:21:49] Fetching from r/tech (Category: Tech) [Attempt 1]
[2025-11-08 17:21:57] Retrieved 181 new records from r/tech.
[2025-11-08 17:22:05] Fetching from r/gadgets (Category: Tech) [Attempt 1]
[2025-11-08 17:22:26] Retrieved 419 new records from r/gadgets.
[2025-11-08 17:22:34] Fetching from r/apple (Category: Tech) [Attempt 1]
[2025-11-08 17:23:27] Retrieved 726 new records from r/apple.
[2025-11-08 17:23:35] Fetching from r/linux (Category: Tech) [Attempt 1]
[2025-11-08 17:24:19] Retrieved 988 new records from r/linux.
[2025-11-08 17:24:30] Data saved: 18952 new rows added. Total: 101117 records.
[2025-11-08 17:24:30] Data collection completed successfully!
[2025-11-08 17:24:30] data_collection.py completed successfully.
[2025-11-08 17:24:30] 
üîπ Running Step: Data Cleaning
[2025-11-08 17:24:30] Starting data_clean.py (Attempt 1)...
[2025-11-08 17:24:32] Loaded dataset with 101117 rows.
[2025-11-08 17:24:51] Cleaned dataset saved with 2591 rows to data\reddit_data_clean.csv.
[2025-11-08 17:24:51] Top keywords found:
keywords_found
linux               1405
azure                291
firewall             223
clio                 209
aws                  193
westlaw               61
active directory      59
docker                48
jira                  42
filevine              42
[2025-11-08 17:24:51] Data cleaning completed successfully.
[2025-11-08 17:24:51] data_clean.py completed successfully.
[2025-11-08 17:24:51] 
üîπ Running Step: Sentiment Analysis
[2025-11-08 17:24:51] Starting data_sentiment.py (Attempt 1)...
Traceback (most recent call last):
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_sentiment.py", line 91, in <module>
    main()
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_sentiment.py", line 36, in main
    log("\U0001f50d Loading NLTK components...")
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_sentiment.py", line 25, in log
    print(msg, flush=True)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f50d' in position 22: character maps to <undefined>
[2025-11-08 17:24:55] data_sentiment.py failed (Attempt 1/2). Retrying...
[2025-11-08 17:25:00] Starting data_sentiment.py (Attempt 2)...
Traceback (most recent call last):
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_sentiment.py", line 91, in <module>
    main()
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_sentiment.py", line 36, in main
    log("\U0001f50d Loading NLTK components...")
  File "C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\data_sentiment.py", line 25, in log
    print(msg, flush=True)
  File "C:\Users\jonis\anaconda3\envs\reditbot\lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\U0001f50d' in position 22: character maps to <undefined>
[2025-11-08 17:25:04] data_sentiment.py failed (Attempt 2/2). Retrying...
[2025-11-08 17:25:09] data_sentiment.py failed after 2 attempts.
[2025-11-08 17:25:09] Step 'Sentiment Analysis' encountered an error. Continuing...

[2025-11-08 17:25:09] 
üîπ Running Step: Indexing to Pinecone
[2025-11-08 17:25:09] Starting store_index.py (Attempt 1)...
C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\langchain_pinecone\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.

For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`
with: `from pydantic import BaseModel`
or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. 	from pydantic.v1 import BaseModel

  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\store_index.py:8: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:

>> from langchain.embeddings import HuggingFaceEmbeddings

with new imports of:

>> from langchain_community.embeddings import HuggingFaceEmbeddings
You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>
  from langchain.embeddings import HuggingFaceEmbeddings
Initializing database connection...
Loaded 1377 total rows; 18952 unindexed IDs in tracker.
No new data to index. Exiting early.
[2025-11-08 17:25:21] store_index.py completed successfully.
[2025-11-08 17:25:21] 
Pipeline finished in 21.1 minutes.
[2025-11-08 17:25:21] ======================================================================
[2025-11-08 17:28:22] ======================================================================
[2025-11-08 17:28:22] Starting Full Data Pipeline
[2025-11-08 17:28:22] ======================================================================
[2025-11-08 17:28:22] 
Running Step: Data Collection
[2025-11-08 17:28:22] Starting data_collection.py (Attempt 1)...
[2025-11-08 17:28:25] Starting Reddit data collection with duplicate prevention...
[2025-11-08 17:28:25] Fetching from r/legaltech (Category: Law) [Attempt 1]
[2025-11-08 17:28:27] No new posts found for r/legaltech.
[2025-11-08 17:28:35] Fetching from r/LawFirm (Category: Law) [Attempt 1]
[2025-11-08 17:28:36] No new posts found for r/LawFirm.
[2025-11-08 17:28:44] Fetching from r/Law (Category: Law) [Attempt 1]
[2025-11-08 17:28:45] No new posts found for r/Law.
[2025-11-08 17:28:53] Fetching from r/LegalAdvice (Category: Law) [Attempt 1]
[2025-11-08 17:28:55] Retrieved 5 new records from r/LegalAdvice.
[2025-11-08 17:29:03] Fetching from r/LegalAdviceUK (Category: Law) [Attempt 1]
[2025-11-08 17:29:05] Retrieved 3 new records from r/LegalAdviceUK.
[2025-11-08 17:29:13] Fetching from r/LegalAdviceCanada (Category: Law) [Attempt 1]
[2025-11-08 17:29:14] No new posts found for r/LegalAdviceCanada.
[2025-11-08 17:29:22] Fetching from r/Paralegal (Category: Law) [Attempt 1]
[2025-11-08 17:29:23] No new posts found for r/Paralegal.
[2025-11-08 17:29:31] Fetching from r/LawSchool (Category: Law) [Attempt 1]
[2025-11-08 17:29:33] Retrieved 2 new records from r/LawSchool.
[2025-11-08 17:29:41] Fetching from r/LegalNews (Category: Law) [Attempt 1]
[2025-11-08 17:29:42] No new posts found for r/LegalNews.
[2025-11-08 17:29:50] Fetching from r/Construction (Category: Construction) [Attempt 1]
[2025-11-08 17:29:53] No new posts found for r/Construction.
[2025-11-08 17:30:01] Fetching from r/Contractors (Category: Construction) [Attempt 1]
[2025-11-08 17:30:01] No new posts found for r/Contractors.
[2025-11-08 17:30:09] Fetching from r/HomeImprovement (Category: Construction) [Attempt 1]
[2025-11-08 17:30:10] Retrieved 3 new records from r/HomeImprovement.
[2025-11-08 17:30:18] Fetching from r/DIY (Category: Construction) [Attempt 1]
[2025-11-08 17:30:20] No new posts found for r/DIY.
[2025-11-08 17:30:28] Fetching from r/DIYChatRoom (Category: Construction) [Attempt 1]
[2025-11-08 17:30:28] Attempt 1 failed for r/DIYChatRoom: Redirect to /subreddits/search
[2025-11-08 17:30:38] Fetching from r/DIYChatRoom (Category: Construction) [Attempt 2]
[2025-11-08 17:30:38] Attempt 2 failed for r/DIYChatRoom: Redirect to /subreddits/search
[2025-11-08 17:30:48] Fetching from r/DIYChatRoom (Category: Construction) [Attempt 3]
[2025-11-08 17:30:48] Attempt 3 failed for r/DIYChatRoom: Redirect to /subreddits/search
[2025-11-08 17:30:58] Skipping r/DIYChatRoom after 3 failed attempts.
[2025-11-08 17:30:58] Fetching from r/Electrical (Category: Construction) [Attempt 1]
[2025-11-08 17:31:01] Retrieved 3 new records from r/Electrical.
[2025-11-08 17:31:09] Fetching from r/Plumbing (Category: Construction) [Attempt 1]
[2025-11-08 17:31:11] Retrieved 1 new records from r/Plumbing.
[2025-11-08 17:31:19] Fetching from r/HVAC (Category: Construction) [Attempt 1]
[2025-11-08 17:31:21] Retrieved 1 new records from r/HVAC.
[2025-11-08 17:31:29] Fetching from r/AskEngineers (Category: Construction) [Attempt 1]
[2025-11-08 17:31:30] No new posts found for r/AskEngineers.
[2025-11-08 17:31:38] Fetching from r/sysadmin (Category: Tech) [Attempt 1]
[2025-11-08 17:31:39] No new posts found for r/sysadmin.
[2025-11-08 17:31:47] Fetching from r/msp (Category: Tech) [Attempt 1]
[2025-11-08 17:31:48] No new posts found for r/msp.
[2025-11-08 17:31:56] Fetching from r/talesfromtechsupport (Category: Tech) [Attempt 1]
[2025-11-08 17:31:57] No new posts found for r/talesfromtechsupport.
[2025-11-08 17:32:05] Fetching from r/iiiiiiitttttttttttt (Category: Tech) [Attempt 1]
[2025-11-08 17:32:07] No new posts found for r/iiiiiiitttttttttttt.
[2025-11-08 17:32:15] Fetching from r/techsupportgore (Category: Tech) [Attempt 1]
[2025-11-08 17:32:16] No new posts found for r/techsupportgore.
[2025-11-08 17:32:24] Fetching from r/ITCareerQuestions (Category: Tech) [Attempt 1]
[2025-11-08 17:32:25] No new posts found for r/ITCareerQuestions.
[2025-11-08 17:32:33] Fetching from r/netsec (Category: Tech) [Attempt 1]
[2025-11-08 17:32:34] No new posts found for r/netsec.
[2025-11-08 17:32:42] Fetching from r/cybersecurity (Category: Tech) [Attempt 1]
[2025-11-08 17:32:43] No new posts found for r/cybersecurity.
[2025-11-08 17:32:52] Fetching from r/technology (Category: Tech) [Attempt 1]
[2025-11-08 17:32:53] No new posts found for r/technology.
[2025-11-08 17:33:01] Fetching from r/tech (Category: Tech) [Attempt 1]
[2025-11-08 17:33:02] No new posts found for r/tech.
[2025-11-08 17:33:10] Fetching from r/gadgets (Category: Tech) [Attempt 1]
[2025-11-08 17:33:11] No new posts found for r/gadgets.
[2025-11-08 17:33:19] Fetching from r/apple (Category: Tech) [Attempt 1]
[2025-11-08 17:33:20] No new posts found for r/apple.
[2025-11-08 17:33:28] Fetching from r/linux (Category: Tech) [Attempt 1]
[2025-11-08 17:33:30] No new posts found for r/linux.
[2025-11-08 17:33:40] Data saved: 18 new rows added. Total: 101135 records.
[2025-11-08 17:33:40] Data collection completed successfully!
[2025-11-08 17:33:40] data_collection.py completed successfully.
[2025-11-08 17:33:40] 
Running Step: Data Cleaning
[2025-11-08 17:33:40] Starting data_clean.py (Attempt 1)...
[2025-11-08 17:33:43] Loaded dataset with 101135 rows.
[2025-11-08 17:34:03] Cleaned dataset saved with 2591 rows to data\reddit_data_clean.csv.
[2025-11-08 17:34:03] Top keywords found:
keywords_found
linux               1405
azure                291
firewall             223
clio                 209
aws                  193
westlaw               61
active directory      59
docker                48
jira                  42
filevine              42
[2025-11-08 17:34:03] Data cleaning completed successfully.
[2025-11-08 17:34:03] data_clean.py completed successfully.
[2025-11-08 17:34:03] 
Running Step: Sentiment Analysis
[2025-11-08 17:34:03] Starting data_sentiment.py (Attempt 1)...
[2025-11-08 17:34:06] Loading NLTK components...
[2025-11-08 17:34:07] Loaded cleaned dataset with 2591 rows.
[2025-11-08 17:34:14] Saved sentiment dataset with 2591 rows to data\reddit_data_sentiment.csv.
[2025-11-08 17:34:14] Sentiment distribution:
sentiment
positive    1736
negative     512
neutral      343
[2025-11-08 17:34:14] Top negative keywords:
   word  count
  linux    408
windows    223
    use    172
   dont    160
     im    134
   like    125
    get    123
   time    121
   work    119
    one    118
  would    106
   need     97
   even     97
  using     93
 people     89
[2025-11-08 17:34:14] Sentiment analysis completed successfully.
[2025-11-08 17:34:15] data_sentiment.py completed successfully.
[2025-11-08 17:34:15] 
Running Step: Indexing to Pinecone
[2025-11-08 17:34:15] Starting store_index.py (Attempt 1)...
C:\Users\jonis\anaconda3\envs\reditbot\lib\site-packages\langchain_pinecone\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.

For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`
with: `from pydantic import BaseModel`
or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. 	from pydantic.v1 import BaseModel

  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\store_index.py:8: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:

>> from langchain.embeddings import HuggingFaceEmbeddings

with new imports of:

>> from langchain_community.embeddings import HuggingFaceEmbeddings
You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>
  from langchain.embeddings import HuggingFaceEmbeddings
Initializing database connection...
Loaded 2591 total rows; 18970 unindexed IDs in tracker.
Preparing 498 new documents for indexing...
C:\Users\jonis\Documents\class\MSDS692_X40_Data Science Practicum II RE\store_index.py:70: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.
  embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
Uploading embeddings to Pinecone...
Upload complete. Marking documents as indexed...
Indexed 498 new documents successfully.
[2025-11-08 17:36:20] store_index.py completed successfully.
[2025-11-08 17:36:20] 
Pipeline finished in 7.97 minutes.
[2025-11-08 17:36:20] ======================================================================
[2025-11-08 18:09:16] ======================================================================
[2025-11-08 18:09:16] Starting Full Data Pipeline
[2025-11-08 18:09:16] ======================================================================
[2025-11-08 18:09:16] 
Running Step: Data Collection
[2025-11-08 18:09:16] Starting data_collection.py (Attempt 1)...
[2025-11-08 18:09:19] Starting Reddit data collection with duplicate prevention...
[2025-11-08 18:09:19] Fetching from r/legaltech (Category: Law) [Attempt 1]
[2025-11-08 18:09:21] No new posts found for r/legaltech.
[2025-11-08 18:34:12] ======================================================================
[2025-11-08 18:34:12] Starting Full Data Pipeline
[2025-11-08 18:34:12] ======================================================================
[2025-11-08 18:34:12] 
Running Step: Data Collection
[2025-11-08 18:34:12] Starting data_collection.py (Attempt 1)...
[2025-11-08 18:34:14] Starting Reddit data collection with duplicate prevention...
[2025-11-08 18:34:14] Fetching from r/legaltech (Category: Law) [Attempt 1]
[2025-11-21 01:09:26] ======================================================================
[2025-11-21 01:09:26] Starting Full Data Pipeline
[2025-11-21 01:09:26] ======================================================================
[2025-11-21 01:09:26] 
Running Step: Data Collection
[2025-11-21 01:09:26] Starting data_collection.py (Attempt 1)...
[2025-11-21 01:09:31] Starting Reddit data collection with duplicate prevention...
[2025-11-21 01:09:31] Fetching from r/legaltech (Category: Law) [Attempt 1]
[2025-11-21 01:09:42] Retrieved 336 new records from r/legaltech.
[2025-11-21 01:09:50] Fetching from r/LawFirm (Category: Law) [Attempt 1]
[2025-11-21 01:10:25] Retrieved 738 new records from r/LawFirm.
[2025-11-21 01:10:33] Fetching from r/Law (Category: Law) [Attempt 1]
[2025-11-21 01:11:26] Retrieved 988 new records from r/Law.
[2025-11-21 01:11:34] Fetching from r/LegalAdvice (Category: Law) [Attempt 1]
[2025-11-21 01:12:26] Retrieved 414 new records from r/LegalAdvice.
[2025-11-21 01:12:34] Fetching from r/LegalAdviceUK (Category: Law) [Attempt 1]
[2025-11-21 01:13:27] Retrieved 710 new records from r/LegalAdviceUK.
[2025-11-21 01:13:35] Fetching from r/LegalAdviceCanada (Category: Law) [Attempt 1]
[2025-11-21 01:14:28] Retrieved 908 new records from r/LegalAdviceCanada.
[2025-11-21 01:14:36] Fetching from r/Paralegal (Category: Law) [Attempt 1]
[2025-11-21 01:15:28] Retrieved 958 new records from r/Paralegal.
[2025-11-21 01:15:36] Fetching from r/LawSchool (Category: Law) [Attempt 1]
[2025-11-21 01:16:28] Retrieved 812 new records from r/LawSchool.
[2025-11-21 01:16:36] Fetching from r/LegalNews (Category: Law) [Attempt 1]
[2025-11-21 01:17:30] Retrieved 744 new records from r/LegalNews.
[2025-11-21 01:17:38] Fetching from r/Construction (Category: Construction) [Attempt 1]
